{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89695a9f-b52e-404a-8ddf-d54de7be0c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import awswrangler as wr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "import warnings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.options.mode.chained_assignment = None\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "226ef930-9a6a-44be-a612-f0e9beb9f8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today I'm going to use cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Today I'm going to use {device.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b41c8e03-7134-4bb8-a50a-d05c75217307",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "N_SAMPLES = 66000\n",
    "TODAY = datetime.today().strftime(\"%Y%m%d\")\n",
    "BUCKET_NAME = 'sagemaker-godeltech'\n",
    "TRAIN_PATH = f\"s3://{BUCKET_NAME}/data/train/train.csv\"\n",
    "VAL_PATH = f\"s3://{BUCKET_NAME}/data/validate/validate.csv\"\n",
    "TEST_PATH = f\"s3://{BUCKET_NAME}/data/test/test.csv\"\n",
    "MODEL_PATH = \"local_transformers/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "527db98c-c6f2-4ed5-a3b1-6f18f70dcdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# del model\n",
    "# del Trainer\n",
    "# del tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7a48123-f123-4802-9578-b93977cb1e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = wr.s3.read_csv([TRAIN_PATH])\n",
    "val = wr.s3.read_csv([VAL_PATH])\n",
    "test = wr.s3.read_csv([TEST_PATH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b59afc66-ea22-4593-916a-80494f20f08e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1443900, 2), (360975, 2), (194641, 12))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample = train.sample(N_SAMPLES, random_state=SEED, ignore_index=True)\n",
    "val_sample = val.sample(N_SAMPLES, random_state=SEED, ignore_index=True)\n",
    "train.shape, val.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6e2ab41-6d29-4ac3-9671-5346394aeedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample['toxicity'] = train_sample['toxicity'].astype('int')\n",
    "val_sample['toxicity'] = val_sample['toxicity'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b8378f8-c775-4f14-82b6-e26d66fc5c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c3f95e0c5942f0b9c697f9f2af7dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa3c0e231ef49cf9a9d099b8e68a01d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd9c49e69964d4bb3f904a44f6bc6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c40f9f10ac4ffaa018e9b5aeb3a653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "715bd8dcae1d44df9ec61df0f4e9a938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164a8834be6e48cfb8c3fce4f75e3641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\", cache_dir = '../tmp/AutoTokenizer');\n",
    "\n",
    "# create tokenization function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"comment_text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# tokenize train and test datasets\n",
    "train_dataset = Dataset.from_pandas(train_sample).map(tokenize, batched=True)\n",
    "val_dataset = Dataset.from_pandas(val_sample).map(tokenize, batched=True)\n",
    "\n",
    "# set dataset format for PyTorch\n",
    "train_dataset =  train_dataset.rename_column(\"toxicity\", \"labels\")\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "val_dataset = val_dataset.rename_column(\"toxicity\", \"labels\")\n",
    "val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cf2ab22-e187-42a4-a54a-49de902a4d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2edebca98b4a42da9686223cfa4eb2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e94a3c6eca44739a14d607dd45275f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/256M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2, cache_dir = '../tmp/AutoModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea4e9fea-6d3e-4240-b214-460f24243c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: comment_text. If comment_text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 66000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2063\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2063' max='2063' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2063/2063 2:10:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.184700</td>\n",
       "      <td>0.145882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.146300</td>\n",
       "      <td>0.134080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.139600</td>\n",
       "      <td>0.134594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.135000</td>\n",
       "      <td>0.126562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: comment_text. If comment_text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../tmp/results/checkpoint-500\n",
      "Configuration saved in ../tmp/results/checkpoint-500/config.json\n",
      "Model weights saved in ../tmp/results/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ../tmp/results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ../tmp/results/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: comment_text. If comment_text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../tmp/results/checkpoint-1000\n",
      "Configuration saved in ../tmp/results/checkpoint-1000/config.json\n",
      "Model weights saved in ../tmp/results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ../tmp/results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ../tmp/results/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: comment_text. If comment_text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../tmp/results/checkpoint-1500\n",
      "Configuration saved in ../tmp/results/checkpoint-1500/config.json\n",
      "Model weights saved in ../tmp/results/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ../tmp/results/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ../tmp/results/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: comment_text. If comment_text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../tmp/results/checkpoint-2000\n",
      "Configuration saved in ../tmp/results/checkpoint-2000/config.json\n",
      "Model weights saved in ../tmp/results/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ../tmp/results/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ../tmp/results/checkpoint-2000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ../tmp/results/checkpoint-2000 (score: 0.12656164169311523).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2063, training_loss=0.1512481760643549, metrics={'train_runtime': 7808.2012, 'train_samples_per_second': 8.453, 'train_steps_per_second': 0.264, 'total_flos': 8742848311296000.0, 'train_loss': 0.1512481760643549, 'epoch': 1.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../tmp/results\",\n",
    "    logging_dir=\"../tmp/results/logs\",\n",
    "    evaluation_strategy = \"steps\",\n",
    "    save_strategy = \"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    seed=SEED,\n",
    "    load_best_model_at_end=True,\n",
    "    overwrite_output_dir=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79b139f9-517e-4e46-93bf-96af1ee18b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = test[['comment_text', 'toxicity']][:10000]\n",
    "test_text['toxicity'] = test_text['toxicity'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a7d4f81-1d23-47f9-a1dd-5081019b5c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc9b1314e1b4b9fb995bbfd1e6c7830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize train and test datasets\n",
    "test_dataset = Dataset.from_pandas(test_text).map(tokenize, batched=True)\n",
    "\n",
    "# set dataset format for PyTorch\n",
    "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5989258-e953-4dc8-a30e-640f4dc7789e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: toxicity, comment_text. If toxicity, comment_text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d377d0e2-a69d-492c-a5dd-e8d29625d705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import save_to_s3\n",
    "\n",
    "y_pred = outputs.predictions.argmax(1)\n",
    "np.savetxt(f\"../tmp/transformers_predictions{TODAY}.csv\", y_pred, delimiter=\",\")\n",
    "save_to_s3(BUCKET_NAME, f\"../tmp/transformers_predictions{TODAY}.csv\", f\"{MODEL_PATH}/transformers_predictions{TODAY}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d8965e7-e081-493b-a3df-547d9cdc03ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_true = test[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ab41c2f-4931-407c-af82-ad5a780e4e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subgroup</th>\n",
       "      <th>subgroup_size</th>\n",
       "      <th>subgroup_auc</th>\n",
       "      <th>bpsn_auc</th>\n",
       "      <th>bnsp_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>psychiatric_or_mental_illness</td>\n",
       "      <td>14</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.657833</td>\n",
       "      <td>0.698810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>homosexual_gay_or_lesbian</td>\n",
       "      <td>13</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.656883</td>\n",
       "      <td>0.808658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>71</td>\n",
       "      <td>0.627941</td>\n",
       "      <td>0.683114</td>\n",
       "      <td>0.843868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>black</td>\n",
       "      <td>36</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.656699</td>\n",
       "      <td>0.873923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>white</td>\n",
       "      <td>68</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.701538</td>\n",
       "      <td>0.863362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>74</td>\n",
       "      <td>0.722034</td>\n",
       "      <td>0.809693</td>\n",
       "      <td>0.812613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>christian</td>\n",
       "      <td>24</td>\n",
       "      <td>0.722689</td>\n",
       "      <td>0.836092</td>\n",
       "      <td>0.784588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jewish</td>\n",
       "      <td>11</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.906124</td>\n",
       "      <td>0.918987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>muslim</td>\n",
       "      <td>26</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.905846</td>\n",
       "      <td>0.922233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        subgroup  subgroup_size  subgroup_auc  bpsn_auc  \\\n",
       "8  psychiatric_or_mental_illness             14      0.458333  0.657833   \n",
       "2      homosexual_gay_or_lesbian             13      0.568182  0.656883   \n",
       "0                           male             71      0.627941  0.683114   \n",
       "6                          black             36      0.633333  0.656699   \n",
       "7                          white             68      0.666667  0.701538   \n",
       "1                         female             74      0.722034  0.809693   \n",
       "3                      christian             24      0.722689  0.836092   \n",
       "4                         jewish             11      0.928571  0.906124   \n",
       "5                         muslim             26      0.931818  0.905846   \n",
       "\n",
       "   bnsp_auc  \n",
       "8  0.698810  \n",
       "2  0.808658  \n",
       "0  0.843868  \n",
       "6  0.873923  \n",
       "7  0.863362  \n",
       "1  0.812613  \n",
       "3  0.784588  \n",
       "4  0.918987  \n",
       "5  0.922233  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL SCORE FOR CUSTOM TRANSFORMERS IS 0.7618243654295905\n"
     ]
    }
   ],
   "source": [
    "from quality_calculator import compute_bias_metrics_for_model, calculate_overall_auc, get_final_metric\n",
    "\n",
    "\n",
    "oof_name = 'predicted_target'\n",
    "identity_columns = ['male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish', 'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "test_true[oof_name] = y_pred\n",
    "#evaluation\n",
    "bias_metrics_df = compute_bias_metrics_for_model(test_true, identity_columns, oof_name, 'toxicity')\n",
    "display(bias_metrics_df)\n",
    "FINAL_SCORE = get_final_metric(bias_metrics_df, calculate_overall_auc(test_true, oof_name))\n",
    "print(f\"FINAL SCORE FOR CUSTOM TRANSFORMERS IS {FINAL_SCORE}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e125e564-6dd8-44f3-94de-ea8ed600a1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Godel",
   "language": "python",
   "name": "godel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
