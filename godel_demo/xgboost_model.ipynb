{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0823c3e2-70ac-4074-97f4-584c732a675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import awswrangler as wr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import models\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.matutils import corpus2dense, corpus2csc\n",
    "from tqdm import tqdm\n",
    "from utils import setup_applevel_logger, get_logger, replace_typical_misspell, clean_text, clean_numbers\n",
    "from utils import save_to_s3, get_from_s3\n",
    "from datetime import datetime\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
    "from quality_calculator import compute_bias_metrics_for_model, calculate_overall_auc, get_final_metric\n",
    "import optuna\n",
    "import warnings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.options.mode.chained_assignment = None\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6339b711-58f9-4247-a1c4-4849206e84c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TODAY = datetime.today().strftime(\"%Y%m%d\")\n",
    "BUCKET_NAME = 'sagemaker-godeltech'\n",
    "TRAIN_PATH = f\"s3://{BUCKET_NAME}/data/train/train.csv\"\n",
    "VAL_PATH = f\"s3://{BUCKET_NAME}/data/validate/validate.csv\"\n",
    "TEST_PATH = f\"s3://{BUCKET_NAME}/data/test/test.csv\"\n",
    "DICTIONARY_PATH = \"xgboost/dictionary\"\n",
    "MODEL_PATH = \"xgboost/models\"\n",
    "SEED = 1234\n",
    "N_SAMPLES = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7ed2fb4-d4d5-45ff-a695-aca77c1273a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = wr.s3.read_csv([TRAIN_PATH])\n",
    "val = wr.s3.read_csv([VAL_PATH])\n",
    "test = wr.s3.read_csv([TEST_PATH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35abbd7d-fd4d-4d34-925f-f5ce34ef87d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = train.sample(N_SAMPLES, random_state=SEED, ignore_index=True)\n",
    "val_sample = val.sample(N_SAMPLES, random_state=SEED, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11aa5e94-d31e-4fea-a895-9cc9675b6695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1443900, 2), (360975, 2), (194641, 12))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, val.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "442b5002-7e2d-4124-8bb4-8568c4bfd0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train_sample['comment_text']\n",
    "val_text = val_sample['comment_text']\n",
    "train_label = train_sample['toxicity']\n",
    "val_label = val_sample['toxicity']\n",
    "test_text = test['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "290a5ce3-4df0-4b84-a6bd-779a0ebfd052",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0dd1524-0fae-4fb6-b77e-37a8e50c7f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.3 s, sys: 1.24 s, total: 34.6 s\n",
      "Wall time: 34.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def simple_preproc(text):\n",
    "    \"\"\"\n",
    "    It is a generator to preprocess texts.\n",
    "    This lowercases, tokenizes, de-accents (optional) \n",
    "    the output are final tokens = unicode strings, that wonâ€™t be processed any further.\n",
    "    \"\"\"\n",
    "    for line in text:\n",
    "        yield simple_preprocess(line)\n",
    "\n",
    "\n",
    "\n",
    "bow_train = [dictionary.doc2bow(doc, allow_update=True) for doc in simple_preproc(train_text)]\n",
    "bow_val = [dictionary.doc2bow(doc, allow_update=False) for doc in simple_preproc(val_text)]\n",
    "bow_test = [dictionary.doc2bow(doc, allow_update=False) for doc in simple_preproc(test_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57ee7d62-2e07-46b6-9aa3-ec7197d35659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs is 100000, there are 72759 words in dictionary\n"
     ]
    }
   ],
   "source": [
    "num_docs = dictionary.num_docs\n",
    "num_terms = len(dictionary.keys())\n",
    "print(f\"Number of docs is {num_docs}, there are {num_terms} words in dictionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81407ee2-babd-4b2a-ba92-390f9fa09fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.6 s, sys: 1.1 s, total: 25.7 s\n",
      "Wall time: 25.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tfidf = models.TfidfModel(bow_train, dictionary=dictionary)\n",
    "train_tfidf = tfidf[bow_train]\n",
    "val_tfidf = tfidf[bow_val]\n",
    "test_tfidf = tfidf[bow_test]\n",
    "train_tfidf_sparse = corpus2csc(train_tfidf, num_terms=num_terms, num_docs=num_docs).T\n",
    "val_tfidf_sparse = corpus2csc(val_tfidf, num_terms=num_terms).T\n",
    "test_tfidf_sparse = corpus2csc(test_tfidf, num_terms=num_terms).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "317cf733-56c3-4d8b-9cf2-a58885555d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tip: if run on GPU than XGBRegressor(tree_method='gpu_hist', gpu_id=0)\n",
    "\n",
    "BEST_SCORE = 0.0\n",
    "\n",
    "def objective(trial):\n",
    "    dtrain = xgb.DMatrix(train_tfidf_sparse, train_label)\n",
    "    dvalid = xgb.DMatrix(val_tfidf_sparse, val_label)\n",
    "    \n",
    "    global BEST_SCORE\n",
    "    param = {\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        # use exact for small dataset.\n",
    "        \"tree_method\": \"exact\",\n",
    "        # defines booster, gblinear for linear functions.\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        # L2 regularization weight.\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        # L1 regularization weight.\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        # sampling ratio for training data.\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        # sampling according to each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "    }\n",
    "    if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        # maximum depth of the tree, signifies complexity of the tree.\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "        # defines how selective algorithm is.\n",
    "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if param[\"booster\"] == \"dart\":\n",
    "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "\n",
    "    bst = xgb.train(param, dtrain)\n",
    "    preds = bst.predict(dvalid)\n",
    "    pred_labels = np.rint(preds)\n",
    "    final_score = roc_auc_score(val_label, pred_labels)\n",
    "    \n",
    "    return final_score\n",
    "\n",
    "optuna.logging.set_verbosity(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "176b497a-567d-4b1b-9537-8a259d85fd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  50\n",
      "Best trial:\n",
      "Value: 0.7301026807114539\n",
      "Params: \n",
      "  booster: gblinear\n",
      "  lambda: 5.790750836239364e-08\n",
      "  alpha: 2.1646078362874597e-06\n",
      "  subsample: 0.2812164532284933\n",
      "  colsample_bytree: 0.31135975873656624\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50, timeout=600)\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"Value: {}\".format(trial.value))\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"  {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa2b3a6a-a47b-4127-a13d-f794f182f66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c58eb56c-9ffe-427a-b0ea-6158dbf49176",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train['comment_text']\n",
    "val_text = val['comment_text']\n",
    "train_label = train['toxicity']\n",
    "val_label = val['toxicity']\n",
    "test_text = test['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd29583a-eafa-487c-93f1-561e0a371402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs is 1443900, there are 248642 words in dictionary\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary()\n",
    "bow_train = [dictionary.doc2bow(doc, allow_update=True) for doc in simple_preproc(train_text)]\n",
    "bow_val = [dictionary.doc2bow(doc, allow_update=False) for doc in simple_preproc(val_text)]\n",
    "bow_test = [dictionary.doc2bow(doc, allow_update=False) for doc in simple_preproc(test_text)]\n",
    "num_docs = dictionary.num_docs\n",
    "num_terms = len(dictionary.keys())\n",
    "print(f\"Number of docs is {num_docs}, there are {num_terms} words in dictionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab09695b-a307-4b0b-aa27-16639aeced1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(bow_train, dictionary=dictionary)\n",
    "train_tfidf = tfidf[bow_train]\n",
    "val_tfidf = tfidf[bow_val]\n",
    "test_tfidf = tfidf[bow_test]\n",
    "train_tfidf_sparse = corpus2csc(train_tfidf, num_terms=num_terms, num_docs=num_docs).T\n",
    "val_tfidf_sparse = corpus2csc(val_tfidf, num_terms=num_terms).T\n",
    "test_tfidf_sparse = corpus2csc(test_tfidf, num_terms=num_terms).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bea8e875-833f-4624-adff-bed9030dffba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlad/anaconda3/envs/godel/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 31s, sys: 1.01 s, total: 2min 32s\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "evaluation = [(train_tfidf_sparse, train_label), (val_tfidf_sparse, val_label)]\n",
    "\n",
    "# fit model no training data\n",
    "xgb_model = XGBClassifier(**xgb_params)\n",
    "xgb_model.fit(train_tfidf_sparse, train_label, eval_set=evaluation, verbose=False)\n",
    "xgb_model.save_model(f\"../tmp/xgb_model_{TODAY}.json\")\n",
    "save_to_s3(BUCKET_NAME, f\"../tmp/xgb_model_{TODAY}.json\", f\"{MODEL_PATH}/xgb_model_{TODAY}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c34bcb3-d146-4c14-ab7e-c00660ae9eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subgroup</th>\n",
       "      <th>subgroup_size</th>\n",
       "      <th>subgroup_auc</th>\n",
       "      <th>bpsn_auc</th>\n",
       "      <th>bnsp_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>homosexual_gay_or_lesbian</td>\n",
       "      <td>1065</td>\n",
       "      <td>0.778182</td>\n",
       "      <td>0.792753</td>\n",
       "      <td>0.953066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>black</td>\n",
       "      <td>1519</td>\n",
       "      <td>0.778235</td>\n",
       "      <td>0.753908</td>\n",
       "      <td>0.968209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>white</td>\n",
       "      <td>2452</td>\n",
       "      <td>0.813015</td>\n",
       "      <td>0.775915</td>\n",
       "      <td>0.970367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>muslim</td>\n",
       "      <td>2040</td>\n",
       "      <td>0.814133</td>\n",
       "      <td>0.815249</td>\n",
       "      <td>0.958486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jewish</td>\n",
       "      <td>835</td>\n",
       "      <td>0.856975</td>\n",
       "      <td>0.856525</td>\n",
       "      <td>0.958015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>psychiatric_or_mental_illness</td>\n",
       "      <td>511</td>\n",
       "      <td>0.882332</td>\n",
       "      <td>0.845194</td>\n",
       "      <td>0.964730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>4386</td>\n",
       "      <td>0.892700</td>\n",
       "      <td>0.882911</td>\n",
       "      <td>0.958298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>5155</td>\n",
       "      <td>0.900830</td>\n",
       "      <td>0.896151</td>\n",
       "      <td>0.955902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>christian</td>\n",
       "      <td>4226</td>\n",
       "      <td>0.905837</td>\n",
       "      <td>0.925548</td>\n",
       "      <td>0.938251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        subgroup  subgroup_size  subgroup_auc  bpsn_auc  \\\n",
       "2      homosexual_gay_or_lesbian           1065      0.778182  0.792753   \n",
       "6                          black           1519      0.778235  0.753908   \n",
       "7                          white           2452      0.813015  0.775915   \n",
       "5                         muslim           2040      0.814133  0.815249   \n",
       "4                         jewish            835      0.856975  0.856525   \n",
       "8  psychiatric_or_mental_illness            511      0.882332  0.845194   \n",
       "0                           male           4386      0.892700  0.882911   \n",
       "1                         female           5155      0.900830  0.896151   \n",
       "3                      christian           4226      0.905837  0.925548   \n",
       "\n",
       "   bnsp_auc  \n",
       "2  0.953066  \n",
       "6  0.968209  \n",
       "7  0.970367  \n",
       "5  0.958486  \n",
       "4  0.958015  \n",
       "8  0.964730  \n",
       "0  0.958298  \n",
       "1  0.955902  \n",
       "3  0.938251  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL SCORE FOR XGBOOST IS 0.8934281875538059\n"
     ]
    }
   ],
   "source": [
    "predictions = xgb_model.predict_proba(test_tfidf_sparse)[:, 1]\n",
    "np.savetxt(f\"../tmp/xgboost_predictions_{TODAY}.csv\", predictions, delimiter=\",\")\n",
    "save_to_s3(BUCKET_NAME, f\"../tmp/xgboost_predictions_{TODAY}.csv\", f\"{MODEL_PATH}/xgboost_predictions_{TODAY}.csv\")\n",
    "oof_name = 'predicted_target'\n",
    "identity_columns = ['male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish', 'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "test[oof_name] = predictions\n",
    "#evaluation\n",
    "bias_metrics_df = compute_bias_metrics_for_model(test, identity_columns, oof_name, 'toxicity')\n",
    "display(bias_metrics_df)\n",
    "FINAL_SCORE = get_final_metric(bias_metrics_df, calculate_overall_auc(test, oof_name))\n",
    "print(f\"FINAL SCORE FOR XGBOOST IS {FINAL_SCORE}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8776d594-7bfb-4750-b545-72dc26906736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Godel",
   "language": "python",
   "name": "godel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
