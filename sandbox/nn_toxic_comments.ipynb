{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96bea07f-6b25-4413-b750-807d556f82de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/huawei-university/nlp-assignment-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26f7750e-496c-434f-99db-850b3b278e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "import gensim.downloader as api\n",
    "\n",
    "import boto3\n",
    "import awswrangler as wr\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import Module, Embedding, LSTM, RNN, GRU, Linear, Sequential, Dropout\n",
    "from torch.nn.functional import sigmoid, relu, elu, tanh\n",
    "from torch.nn.utils.rnn import PackedSequence\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from numpy import asarray\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b62fcd94-f4cf-4fd7-9c1d-60dc32f23224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks = pd.read_csv(\"../data/all_data.csv\", chunksize=100000)\n",
    "# df = pd.concat(chunks)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19608e05-e544-4c3a-b8d0-c60814c707fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['godel-tf-state', 'godelsagemaker', 'sagemaker-eu-west-1-798631296162']\n"
     ]
    }
   ],
   "source": [
    "#list all buckets\n",
    "s3 = boto3.resource('s3')\n",
    "buckets = [bucket.name for bucket in s3.buckets.all()]\n",
    "print(buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c966a8e6-b115-496e-9e90-608df43c835b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>split</th>\n",
       "      <th>created_date</th>\n",
       "      <th>publication_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "      <th>transgender</th>\n",
       "      <th>other_gender</th>\n",
       "      <th>heterosexual</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>other_sexual_orientation</th>\n",
       "      <th>christian</th>\n",
       "      <th>jewish</th>\n",
       "      <th>muslim</th>\n",
       "      <th>hindu</th>\n",
       "      <th>buddhist</th>\n",
       "      <th>atheist</th>\n",
       "      <th>other_religion</th>\n",
       "      <th>black</th>\n",
       "      <th>white</th>\n",
       "      <th>asian</th>\n",
       "      <th>latino</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1083994</td>\n",
       "      <td>He got his money... now he lies in wait till after the election in 2 yrs.... dirty politicians need to be afraid of Tar and feathers again... but they aren't and so the people get screwed.</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-03-06 15:21:53.675241+00</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>317120</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.373134</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>0.089552</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.343284</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>650904</td>\n",
       "      <td>Mad dog will surely put the liberals in mental hospitals. Boorah</td>\n",
       "      <td>train</td>\n",
       "      <td>2016-12-02 16:44:21.329535+00</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>154086</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.065789</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.092105</td>\n",
       "      <td>0.565789</td>\n",
       "      <td>0.065789</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5902188</td>\n",
       "      <td>And Trump continues his lifelong cowardice by not making this announcement himself.\\n\\nWhat an awful human being .....</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-09-05 19:05:32.341360+00</td>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>374342</td>\n",
       "      <td>approved</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7084460</td>\n",
       "      <td>\"while arresting a man for resisting arrest\".\\n\\nIf you cop-suckers can't see a problem with this, then go suck the barrel of a Glock.</td>\n",
       "      <td>test</td>\n",
       "      <td>2016-11-01 16:53:33.561631+00</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>149218</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.065789</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.592105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5410943</td>\n",
       "      <td>Tucker and Paul are both total bad ass mofo's.</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-06-14 05:08:21.997315+00</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>344096</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.337500</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.487500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  \\\n",
       "0  1083994   \n",
       "1   650904   \n",
       "2  5902188   \n",
       "3  7084460   \n",
       "4  5410943   \n",
       "\n",
       "                                                                                                                                                                                   comment_text  \\\n",
       "0  He got his money... now he lies in wait till after the election in 2 yrs.... dirty politicians need to be afraid of Tar and feathers again... but they aren't and so the people get screwed.   \n",
       "1                                                                                                                              Mad dog will surely put the liberals in mental hospitals. Boorah   \n",
       "2                                                                        And Trump continues his lifelong cowardice by not making this announcement himself.\\n\\nWhat an awful human being .....   \n",
       "3                                                        \"while arresting a man for resisting arrest\".\\n\\nIf you cop-suckers can't see a problem with this, then go suck the barrel of a Glock.   \n",
       "4                                                                                                                                                Tucker and Paul are both total bad ass mofo's.   \n",
       "\n",
       "   split                   created_date  publication_id  parent_id  \\\n",
       "0  train  2017-03-06 15:21:53.675241+00              21        NaN   \n",
       "1  train  2016-12-02 16:44:21.329535+00              21        NaN   \n",
       "2  train  2017-09-05 19:05:32.341360+00              55        NaN   \n",
       "3   test  2016-11-01 16:53:33.561631+00              13        NaN   \n",
       "4  train  2017-06-14 05:08:21.997315+00              21        NaN   \n",
       "\n",
       "   article_id    rating  funny  wow  sad  likes  disagree  toxicity  \\\n",
       "0      317120  approved      0    0    0      2         0  0.373134   \n",
       "1      154086  approved      0    0    1      2         0  0.605263   \n",
       "2      374342  approved      1    0    2      3         7  0.666667   \n",
       "3      149218  approved      0    0    0      0         0  0.815789   \n",
       "4      344096  approved      0    0    0      1         0  0.550000   \n",
       "\n",
       "   severe_toxicity   obscene  sexual_explicit  identity_attack    insult  \\\n",
       "0         0.044776  0.089552         0.014925         0.000000  0.343284   \n",
       "1         0.013158  0.065789         0.013158         0.092105  0.565789   \n",
       "2         0.015873  0.031746         0.000000         0.047619  0.666667   \n",
       "3         0.065789  0.552632         0.592105         0.000000  0.684211   \n",
       "4         0.037500  0.337500         0.275000         0.037500  0.487500   \n",
       "\n",
       "     threat  male  female  transgender  other_gender  heterosexual  \\\n",
       "0  0.014925   NaN     NaN          NaN           NaN           NaN   \n",
       "1  0.065789   NaN     NaN          NaN           NaN           NaN   \n",
       "2  0.000000   NaN     NaN          NaN           NaN           NaN   \n",
       "3  0.105263   NaN     NaN          NaN           NaN           NaN   \n",
       "4  0.000000   NaN     NaN          NaN           NaN           NaN   \n",
       "\n",
       "   homosexual_gay_or_lesbian  bisexual  other_sexual_orientation  christian  \\\n",
       "0                        NaN       NaN                       NaN        NaN   \n",
       "1                        NaN       NaN                       NaN        NaN   \n",
       "2                        NaN       NaN                       NaN        NaN   \n",
       "3                        NaN       NaN                       NaN        NaN   \n",
       "4                        NaN       NaN                       NaN        NaN   \n",
       "\n",
       "   jewish  muslim  hindu  buddhist  atheist  other_religion  black  white  \\\n",
       "0     NaN     NaN    NaN       NaN      NaN             NaN    NaN    NaN   \n",
       "1     NaN     NaN    NaN       NaN      NaN             NaN    NaN    NaN   \n",
       "2     NaN     NaN    NaN       NaN      NaN             NaN    NaN    NaN   \n",
       "3     NaN     NaN    NaN       NaN      NaN             NaN    NaN    NaN   \n",
       "4     NaN     NaN    NaN       NaN      NaN             NaN    NaN    NaN   \n",
       "\n",
       "   asian  latino  other_race_or_ethnicity  physical_disability  \\\n",
       "0    NaN     NaN                      NaN                  NaN   \n",
       "1    NaN     NaN                      NaN                  NaN   \n",
       "2    NaN     NaN                      NaN                  NaN   \n",
       "3    NaN     NaN                      NaN                  NaN   \n",
       "4    NaN     NaN                      NaN                  NaN   \n",
       "\n",
       "   intellectual_or_learning_disability  psychiatric_or_mental_illness  \\\n",
       "0                                  NaN                            NaN   \n",
       "1                                  NaN                            NaN   \n",
       "2                                  NaN                            NaN   \n",
       "3                                  NaN                            NaN   \n",
       "4                                  NaN                            NaN   \n",
       "\n",
       "   other_disability  identity_annotator_count  toxicity_annotator_count  \n",
       "0               NaN                         0                        67  \n",
       "1               NaN                         0                        76  \n",
       "2               NaN                         0                        63  \n",
       "3               NaN                         0                        76  \n",
       "4               NaN                         0                        80  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket = 'godelsagemaker'\n",
    "\n",
    "chunks = wr.s3.read_csv(path=f's3://{bucket}/data/toxic_data.csv', chunksize=10000)\n",
    "df = pd.concat(chunks)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7866c72e-6e2c-4377-b458-7459826f1ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.sample(100_000)\n",
    "# sample = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b159910-a419-4cab-904b-8478a166e9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['comment_text'] = sample['comment_text'].fillna('')\n",
    "identity_columns = ['male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish', 'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "for col in identity_columns + ['toxicity']:\n",
    "    sample.loc[:, col] = np.where(sample[col] >= 0.5, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "932fc45a-7e75-4660-8ff6-3833c49c59db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = sample[sample['split'] == 'train']['comment_text'].tolist()\n",
    "test_df = sample[sample['split'] == 'test']['comment_text'].tolist()\n",
    "y_train = sample[sample['split'] == 'train']['toxicity'].astype('int').tolist()\n",
    "y_test = sample[sample['split'] == 'test']['toxicity'].astype('int').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf39c9b3-93de-4d91-b48e-2a4af5b9ff8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "722616\n",
      "77496\n",
      "722616\n",
      "77496\n"
     ]
    }
   ],
   "source": [
    "from sys import getsizeof\n",
    "\n",
    "print(getsizeof(train_df))\n",
    "print(getsizeof(test_df))\n",
    "print(getsizeof(y_train))\n",
    "print(getsizeof(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3337978d-2358-484a-b908-7e4d81be1c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, word_pattern=\"[\\w']+\"):\n",
    "        \"\"\"\n",
    "        Simple tokenizer that splits the sentence by given regex pattern\n",
    "        :param word_pattern: pattern that determines word boundaries\n",
    "        \"\"\"\n",
    "        self.word_pattern = re.compile(word_pattern)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return self.word_pattern.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "865f1e66-1067-478c-ba0f-05bfe25997f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokenized_texts: List[List[str]], max_vocab_size=None):\n",
    "        \"\"\"\n",
    "        Builds a vocabulary by concatenating all tokenized texts and counting words.\n",
    "        Most common words are placed in vocabulary, others are replaced with [UNK] token\n",
    "        :param tokenized_texts: texts to build a vocab\n",
    "        :param max_vocab_size: amount of words in vocabulary\n",
    "        \"\"\"\n",
    "        counts = Counter(chain(*tokenized_texts))\n",
    "        max_vocab_size = max_vocab_size or len(counts)\n",
    "        common_pairs = counts.most_common(max_vocab_size)\n",
    "        self.PAD_IDX = 0\n",
    "        self.UNK_IDX = 1\n",
    "        self.EOS_IDX = 2\n",
    "        self.itos = [\"<PAD>\", \"<UNK>\", \"<EOS>\"] + [pair[0] for pair in common_pairs]\n",
    "        self.stoi = {token: i for i, token in enumerate(self.itos)}\n",
    "\n",
    "    def vectorize(self, text: List[str]):\n",
    "        \"\"\"\n",
    "        Maps each token to it's index in the vocabulary\n",
    "        :param text: sequence of tokens\n",
    "        :return: vectorized sequence\n",
    "        \"\"\"\n",
    "        return [self.stoi.get(tok, self.UNK_IDX) for tok in text]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.itos)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e6b112f-bf2c-4c92-afbf-526280dca537",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenized_texts, labels, vocab: Vocab):\n",
    "        \"\"\"\n",
    "        A Dataset for the task\n",
    "        :param tokenized_texts: texts from a train/val/test split\n",
    "        :param labels: corresponding toxicity ratings\n",
    "        :param vocab: vocabulary with indexed tokens\n",
    "        \"\"\"\n",
    "        self.texts = tokenized_texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.vocab.vectorize(self.texts[item]) + [self.vocab.EOS_IDX], self.labels[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Technical method to form a batch to feed into recurrent network\n",
    "        \"\"\"\n",
    "        return pack_sequence([torch.tensor(pair[0]) for pair in batch], enforce_sorted=False), torch.tensor(\n",
    "            [pair[1] for pair in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55f7eb1c-694a-42e2-a07a-a90171d0cd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer()\n",
    "tok_texts = [tok.tokenize(t) for t in train_df]\n",
    "vocab = Vocab(tok_texts, max_vocab_size=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8047c81-89a7-455e-ba0f-f62a63ce12ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset([tok.tokenize(t) for t in train_df], y_train, vocab)\n",
    "test_dataset = TextDataset([tok.tokenize(t) for t in test_df], y_test, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4e86fce-876b-4b39-a54e-79008ad9f74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_emb_matrix(gensim_model, vocab: Vocab):\n",
    "    \"\"\"\n",
    "    Extract embedding matrix from Gensim model for words in Vocab.\n",
    "    Initialize embeddings not presented in `gensim_model` randomly\n",
    "    :param gensim_model: W2V Gensim model\n",
    "    :param vocab: vocabulary\n",
    "    :return: embedding matrix\n",
    "    \"\"\"\n",
    "    mean = gensim_model.vectors.mean(1).mean()\n",
    "    std = gensim_model.vectors.std(1).mean()\n",
    "    vec_size = gensim_model.vector_size\n",
    "    emb_matrix = torch.zeros((len(vocab), vec_size))\n",
    "    for i, word in enumerate(vocab.itos[1:], 1):\n",
    "        try:\n",
    "            emb_matrix[i] = torch.tensor(gensim_model.get_vector(word))\n",
    "        except KeyError:\n",
    "            emb_matrix[i] = torch.randn(vec_size) * std + mean\n",
    "    return emb_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "846850aa-bbd5-4bca-8363-fd5126d2d063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# store embeddings in current directory\n",
    "os.environ[\"GENSIM_DATA_DIR\"] = str(Path.cwd())\n",
    "# will download embeddings or load them from disk\n",
    "gensim_model = api.load(\"glove-wiki-gigaword-100\")\n",
    "emb_matrix = prepare_emb_matrix(gensim_model, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b01dc80-1b07-4f37-bc20-318286c77335",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentClassifier(Module):\n",
    "    def __init__(self, config: Dict, vocab: Vocab, emb_matrix):\n",
    "        \"\"\"\n",
    "        Baseline classifier, hyperparameters are passed in `config`.\n",
    "        Consists of recurrent part and a classifier (Multilayer Perceptron) part\n",
    "        Keys are:\n",
    "            - freeze: whether word embeddings should be frozen\n",
    "            - cell_type: one of: RNN, GRU, LSTM, which recurrent cell model should use\n",
    "            - hidden_size: size of hidden state for recurrent cell\n",
    "            - num_layers: amount of recurrent cells in the model\n",
    "            - cell_dropout: dropout rate between recurrent cells (not applied if model has only one cell!)\n",
    "            - bidirectional: boolean, whether to use unidirectional of bidirectional model\n",
    "            - out_activation: one of: \"sigmoid\", \"tanh\", \"relu\", \"elu\". Activation in classifier part\n",
    "            - out_dropout: dropout rate in classifier part\n",
    "            - out_sizes: List[int], hidden size of each layer in classifier part. Empty list means that final\n",
    "                layer is attached directly to recurrent part output\n",
    "        :param config: configuration of model\n",
    "        :param vocab: vocabulary\n",
    "        :param emb_matrix: embeddings matrix from `prepare_emb_matrix`\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.vocab = vocab\n",
    "        self.emb_matrix = emb_matrix\n",
    "        self.embeddings = Embedding.from_pretrained(emb_matrix, freeze=config[\"freeze\"],\n",
    "                                                    padding_idx=vocab.PAD_IDX)\n",
    "        cell_types = {\n",
    "            \"RNN\": RNN,\n",
    "            \"GRU\": GRU,\n",
    "            \"LSTM\": LSTM}\n",
    "        cell_class = cell_types[config[\"cell_type\"]]\n",
    "        self.cell = cell_class(input_size=emb_matrix.size(1),\n",
    "                               batch_first=True,\n",
    "                               hidden_size=config[\"hidden_size\"],\n",
    "                               num_layers=config[\"num_layers\"],\n",
    "                               dropout=config[\"cell_dropout\"],\n",
    "                               bidirectional=config[\"bidirectional\"],\n",
    "                               )\n",
    "        activation_types = {\n",
    "            \"sigmoid\": sigmoid,\n",
    "            \"tanh\": tanh,\n",
    "            \"relu\": relu,\n",
    "            \"elu\": elu,\n",
    "        }\n",
    "        self.out_activation = activation_types[config[\"out_activation\"]]\n",
    "        self.out_dropout = Dropout(config[\"out_dropout\"])\n",
    "        cur_out_size = config[\"hidden_size\"] * config[\"num_layers\"]\n",
    "        if config[\"bidirectional\"]:\n",
    "            cur_out_size *= 2\n",
    "        out_layers = []\n",
    "        for cur_hidden_size in config[\"out_sizes\"]:\n",
    "            out_layers.append(Linear(cur_out_size, cur_hidden_size))\n",
    "            cur_out_size = cur_hidden_size\n",
    "        out_layers.append(Linear(cur_out_size, 6))\n",
    "        self.out_proj = Sequential(*out_layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.embeddings(input.data)\n",
    "        _, last_state = self.cell(PackedSequence(embedded,\n",
    "                                                 input.batch_sizes,\n",
    "                                                 sorted_indices=input.sorted_indices,\n",
    "                                                 unsorted_indices=input.unsorted_indices))\n",
    "        if isinstance(last_state, tuple):\n",
    "            last_state = last_state[0]\n",
    "        last_state = last_state.transpose(0, 1)\n",
    "        last_state = last_state.reshape(last_state.size(0), -1)\n",
    "        return self.out_proj(last_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57fb9cf0-e9af-4a20-b9b0-faf2ce98e600",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"freeze\": True,\n",
    "    \"cell_type\": \"LSTM\",\n",
    "    \"cell_dropout\": 0.3,\n",
    "    \"num_layers\": 2,\n",
    "    \"hidden_size\": 128,\n",
    "    \"out_activation\": \"relu\",\n",
    "    \"bidirectional\": False,\n",
    "    \"out_dropout\": 0.2,\n",
    "    \"out_sizes\": [200],\n",
    "}\n",
    "\n",
    "trainer_config = {\n",
    "    \"lr\": 3e-4,\n",
    "    \"n_epochs\": 10,\n",
    "    \"weight_decay\": 1e-6,\n",
    "    \"batch_size\": 128,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "}\n",
    "clf_model = RecurrentClassifier(config, vocab, emb_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7765b791-6041-41a6-b0a8-587b5eeaf8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, config: Dict):\n",
    "        \"\"\"\n",
    "        Fits end evaluates given model with Adam optimizer.\n",
    "         Hyperparameters are specified in `config`\n",
    "        Possible keys are:\n",
    "            - n_epochs: number of epochs to train\n",
    "            - lr: optimizer learning rate\n",
    "            - weight_decay: l2 regularization weight\n",
    "            - device: on which device to perform training (\"cpu\" or \"cuda\")\n",
    "            - verbose: whether to print anything during training\n",
    "        :param config: configuration for `Trainer`\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.n_epochs = config[\"n_epochs\"]\n",
    "        self.setup_opt_fn = lambda model: Adam(model.parameters(),\n",
    "                                               config[\"lr\"],\n",
    "                                               weight_decay=config[\"weight_decay\"])\n",
    "        self.model = None\n",
    "        self.opt = None\n",
    "        self.history = None\n",
    "        self.loss_fn = CrossEntropyLoss()\n",
    "        self.device = config[\"device\"]\n",
    "        self.verbose = config.get(\"verbose\", True)\n",
    "\n",
    "    def fit(self, model, train_loader, val_loader):\n",
    "        \"\"\"\n",
    "        Fits model on training data, each epoch evaluates on validation data\n",
    "        :param model: PyTorch model for toxic comments classification (for example, `RecurrentClassifier`)\n",
    "        :param train_loader: DataLoader for training data\n",
    "        :param val_loader: DataLoader for validation data\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.model = model.to(self.device)\n",
    "        self.opt = self.setup_opt_fn(self.model)\n",
    "        self.history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "        for epoch in range(self.n_epochs):\n",
    "            train_info = self._train_epoch(train_loader)\n",
    "            val_info = self._val_epoch(val_loader)\n",
    "            self.history[\"train_loss\"].extend(train_info[\"train_loss\"])\n",
    "            self.history[\"val_loss\"].append(val_info[\"loss\"])\n",
    "            self.history[\"val_acc\"].append(val_info[\"acc\"])\n",
    "        return self.model.eval()\n",
    "\n",
    "    def _train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "        if self.verbose:\n",
    "            train_loader = tqdm(train_loader)\n",
    "        for batch in train_loader:\n",
    "            self.model.zero_grad()\n",
    "            texts, labels = batch\n",
    "            logits = self.model.forward(texts.to(self.device))\n",
    "            loss = self.loss_fn(logits, labels.to(self.device))\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "            loss_val = loss.item()\n",
    "            if self.verbose:\n",
    "                train_loader.set_description(f\"Loss={loss_val:.3}\")\n",
    "            losses.append(loss_val)\n",
    "        return {\"train_loss\": losses}\n",
    "\n",
    "    def _val_epoch(self, val_loader):\n",
    "        self.model.eval()\n",
    "        all_logits = []\n",
    "        all_labels = []\n",
    "        if self.verbose:\n",
    "            val_loader = tqdm(val_loader)\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                texts, labels = batch\n",
    "                logits = self.model.forward(texts.to(self.device))\n",
    "                all_logits.append(logits)\n",
    "                all_labels.append(labels)\n",
    "        all_labels = torch.cat(all_labels).to(self.device)\n",
    "        all_logits = torch.cat(all_logits)\n",
    "        loss = CrossEntropyLoss()(all_logits, all_labels).item()\n",
    "        acc = (all_logits.argmax(1) == all_labels).float().mean().item()\n",
    "        if self.verbose:\n",
    "            val_loader.set_description(f\"Loss={loss:.3}; Acc:{acc:.3}\")\n",
    "        return {\"acc\": acc, \"loss\": loss}\n",
    "\n",
    "    def predict(self, test_loader):\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"You should train the model first\")\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                texts, labels = batch\n",
    "                logits = self.model.forward(texts.to(self.device))\n",
    "                predictions.extend(logits.argmax(1).tolist())\n",
    "        return asarray(predictions)\n",
    "\n",
    "    def save(self, path: str):\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"You should train the model first\")\n",
    "        checkpoint = {\"config\": self.model.config,\n",
    "                      \"trainer_config\": self.config,\n",
    "                      \"vocab\": self.model.vocab,\n",
    "                      \"emb_matrix\": self.model.emb_matrix,\n",
    "                      \"state_dict\": self.model.state_dict()}\n",
    "        torch.save(checkpoint, path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str):\n",
    "        ckpt = torch.load(path)\n",
    "        keys = [\"config\", \"trainer_config\", \"vocab\", \"emb_matrix\", \"state_dict\"]\n",
    "        for key in keys:\n",
    "            if key not in ckpt:\n",
    "                raise RuntimeError(f\"Missing key {key} in checkpoint\")\n",
    "        new_model = RecurrentClassifier(ckpt[\"config\"], ckpt[\"vocab\"], ckpt[\"emb_matrix\"])\n",
    "        new_model.load_state_dict(ckpt[\"state_dict\"])\n",
    "        new_trainer = cls(ckpt[\"trainer_config\"])\n",
    "        new_trainer.model = new_model\n",
    "        new_trainer.model.to(new_trainer.device)\n",
    "        return new_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7f56c4-53da-4623-9a19-2f3a72758d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.04523468017578125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 706,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d1e804b25f3495e92b18356b106cbbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/706 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02264714241027832,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 76,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeafaf5f40854f048038175098751f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018396615982055664,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 706,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d6439b250af4a43811577066052f39f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/706 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.019342899322509766,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 76,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a750cb9cce8a43feb4f3d52cbaecb4de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01665210723876953,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 706,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0293760e5ee747a1b0571ff2ba2367ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/706 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=trainer_config[\"batch_size\"],\n",
    "                              shuffle=True,\n",
    "                              num_workers=4,\n",
    "                              collate_fn=train_dataset.collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, \n",
    "                            batch_size=trainer_config[\"batch_size\"],\n",
    "                            shuffle=False,\n",
    "                            num_workers=4,\n",
    "                            collate_fn=test_dataset.collate_fn)\n",
    "t = Trainer(trainer_config)\n",
    "t.fit(clf_model, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902ae1ad-d62b-40f8-bd94-7141dd751f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.save(\"baseline_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16900e1-f073-4719-b90d-52a2766e1be9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "godel:Python",
   "language": "python",
   "name": "conda-env-godel-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
