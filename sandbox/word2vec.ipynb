{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f200dd6-dc23-4473-9909-89e161202fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import awswrangler as wr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from tqdm import tqdm \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 1234\n",
    "N_SAMPLES = 10_000\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import gensim.downloader as api\n",
    "import logging \n",
    "# Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "import warnings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.options.mode.chained_assignment = None\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85f53dc6-b405-4b80-bcc1-c0e3c1a70f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>split</th>\n",
       "      <th>created_date</th>\n",
       "      <th>publication_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "      <th>transgender</th>\n",
       "      <th>other_gender</th>\n",
       "      <th>heterosexual</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>other_sexual_orientation</th>\n",
       "      <th>christian</th>\n",
       "      <th>jewish</th>\n",
       "      <th>muslim</th>\n",
       "      <th>hindu</th>\n",
       "      <th>buddhist</th>\n",
       "      <th>atheist</th>\n",
       "      <th>other_religion</th>\n",
       "      <th>black</th>\n",
       "      <th>white</th>\n",
       "      <th>asian</th>\n",
       "      <th>latino</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1083994</td>\n",
       "      <td>He got his money... now he lies in wait till after the election in 2 yrs.... dirty politicians need to be afraid of Tar and feathers again... but they aren't and so the people get screwed.</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-03-06 15:21:53.675241+00</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>317120</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.373134</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>0.089552</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.343284</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>650904</td>\n",
       "      <td>Mad dog will surely put the liberals in mental hospitals. Boorah</td>\n",
       "      <td>train</td>\n",
       "      <td>2016-12-02 16:44:21.329535+00</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>154086</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.065789</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.092105</td>\n",
       "      <td>0.565789</td>\n",
       "      <td>0.065789</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5902188</td>\n",
       "      <td>And Trump continues his lifelong cowardice by not making this announcement himself.\\n\\nWhat an awful human being .....</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-09-05 19:05:32.341360+00</td>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>374342</td>\n",
       "      <td>approved</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7084460</td>\n",
       "      <td>\"while arresting a man for resisting arrest\".\\n\\nIf you cop-suckers can't see a problem with this, then go suck the barrel of a Glock.</td>\n",
       "      <td>test</td>\n",
       "      <td>2016-11-01 16:53:33.561631+00</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>149218</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.065789</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.592105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5410943</td>\n",
       "      <td>Tucker and Paul are both total bad ass mofo's.</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-06-14 05:08:21.997315+00</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>344096</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.337500</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.487500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  \\\n",
       "0  1083994   \n",
       "1   650904   \n",
       "2  5902188   \n",
       "3  7084460   \n",
       "4  5410943   \n",
       "\n",
       "                                                                                                                                                                                   comment_text  \\\n",
       "0  He got his money... now he lies in wait till after the election in 2 yrs.... dirty politicians need to be afraid of Tar and feathers again... but they aren't and so the people get screwed.   \n",
       "1                                                                                                                              Mad dog will surely put the liberals in mental hospitals. Boorah   \n",
       "2                                                                        And Trump continues his lifelong cowardice by not making this announcement himself.\\n\\nWhat an awful human being .....   \n",
       "3                                                        \"while arresting a man for resisting arrest\".\\n\\nIf you cop-suckers can't see a problem with this, then go suck the barrel of a Glock.   \n",
       "4                                                                                                                                                Tucker and Paul are both total bad ass mofo's.   \n",
       "\n",
       "   split                   created_date  publication_id  parent_id  \\\n",
       "0  train  2017-03-06 15:21:53.675241+00              21        NaN   \n",
       "1  train  2016-12-02 16:44:21.329535+00              21        NaN   \n",
       "2  train  2017-09-05 19:05:32.341360+00              55        NaN   \n",
       "3   test  2016-11-01 16:53:33.561631+00              13        NaN   \n",
       "4  train  2017-06-14 05:08:21.997315+00              21        NaN   \n",
       "\n",
       "   article_id    rating  funny  wow  sad  likes  disagree  toxicity  \\\n",
       "0      317120  approved      0    0    0      2         0  0.373134   \n",
       "1      154086  approved      0    0    1      2         0  0.605263   \n",
       "2      374342  approved      1    0    2      3         7  0.666667   \n",
       "3      149218  approved      0    0    0      0         0  0.815789   \n",
       "4      344096  approved      0    0    0      1         0  0.550000   \n",
       "\n",
       "   severe_toxicity   obscene  sexual_explicit  identity_attack    insult  \\\n",
       "0         0.044776  0.089552         0.014925         0.000000  0.343284   \n",
       "1         0.013158  0.065789         0.013158         0.092105  0.565789   \n",
       "2         0.015873  0.031746         0.000000         0.047619  0.666667   \n",
       "3         0.065789  0.552632         0.592105         0.000000  0.684211   \n",
       "4         0.037500  0.337500         0.275000         0.037500  0.487500   \n",
       "\n",
       "     threat  male  female  transgender  other_gender  heterosexual  \\\n",
       "0  0.014925   NaN     NaN          NaN           NaN           NaN   \n",
       "1  0.065789   NaN     NaN          NaN           NaN           NaN   \n",
       "2  0.000000   NaN     NaN          NaN           NaN           NaN   \n",
       "3  0.105263   NaN     NaN          NaN           NaN           NaN   \n",
       "4  0.000000   NaN     NaN          NaN           NaN           NaN   \n",
       "\n",
       "   homosexual_gay_or_lesbian  bisexual  other_sexual_orientation  christian  \\\n",
       "0                        NaN       NaN                       NaN        NaN   \n",
       "1                        NaN       NaN                       NaN        NaN   \n",
       "2                        NaN       NaN                       NaN        NaN   \n",
       "3                        NaN       NaN                       NaN        NaN   \n",
       "4                        NaN       NaN                       NaN        NaN   \n",
       "\n",
       "   jewish  muslim  hindu  buddhist  atheist  other_religion  black  white  \\\n",
       "0     NaN     NaN    NaN       NaN      NaN             NaN    NaN    NaN   \n",
       "1     NaN     NaN    NaN       NaN      NaN             NaN    NaN    NaN   \n",
       "2     NaN     NaN    NaN       NaN      NaN             NaN    NaN    NaN   \n",
       "3     NaN     NaN    NaN       NaN      NaN             NaN    NaN    NaN   \n",
       "4     NaN     NaN    NaN       NaN      NaN             NaN    NaN    NaN   \n",
       "\n",
       "   asian  latino  other_race_or_ethnicity  physical_disability  \\\n",
       "0    NaN     NaN                      NaN                  NaN   \n",
       "1    NaN     NaN                      NaN                  NaN   \n",
       "2    NaN     NaN                      NaN                  NaN   \n",
       "3    NaN     NaN                      NaN                  NaN   \n",
       "4    NaN     NaN                      NaN                  NaN   \n",
       "\n",
       "   intellectual_or_learning_disability  psychiatric_or_mental_illness  \\\n",
       "0                                  NaN                            NaN   \n",
       "1                                  NaN                            NaN   \n",
       "2                                  NaN                            NaN   \n",
       "3                                  NaN                            NaN   \n",
       "4                                  NaN                            NaN   \n",
       "\n",
       "   other_disability  identity_annotator_count  toxicity_annotator_count  \n",
       "0               NaN                         0                        67  \n",
       "1               NaN                         0                        76  \n",
       "2               NaN                         0                        63  \n",
       "3               NaN                         0                        76  \n",
       "4               NaN                         0                        80  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = pd.read_csv('../data/toxic_data.csv', chunksize=100000)\n",
    "df = pd.concat(chunks)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ffde508-8348-4a8d-8e29-90218781ab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment_text'] = df['comment_text'].fillna(\"\")\n",
    "identity_columns = ['male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish', 'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "for col in identity_columns + ['toxicity']:\n",
    "    df.loc[:, col] = np.where(df[col] >= 0.5, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab1c1137-ab5a-46c0-be73-929c958d2f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df['split'] == 'train']\n",
    "test_df = df[df['split'] != 'train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "862d4ed0-97ba-4782-8595-d0ef5336552e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1804875, 46), (194641, 46))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19c3100a-8adb-4b18-9ec4-d5bcb94ca340",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_df.sample(N_SAMPLES, random_state=SEED, ignore_index=True)\n",
    "train_text, train_label = sample['comment_text'], sample['toxicity']\n",
    "test_text, test_label = test_df['comment_text'], test_df['toxicity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8aab7c8-bc84-4788-80f3-d22732f6d94b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (194641,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text.shape, test_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "014e6408-77d2-437b-8616-5d0e7ddad0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 18181.69it/s]\n",
      "100%|██████████| 194641/194641 [00:10<00:00, 17987.64it/s]\n"
     ]
    }
   ],
   "source": [
    "train_lists = [simple_preprocess(doc) for doc in tqdm(train_text)]\n",
    "test_lists = [simple_preprocess(doc) for doc in tqdm(test_text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c141f656-e956-497d-96a3-6365d39fb4bd",
   "metadata": {},
   "source": [
    "## Custom Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10029858-25f0-48b1-bd64-f2ffe9e5ae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#thanks to https://ethen8181.github.io/machine-learning/keras/text_classification/word2vec_text_classification.html\n",
    "\n",
    "class GensimWord2VecVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Word vectors are averaged across to create the document-level vectors/features.\n",
    "    gensim's own gensim.sklearn_api.W2VTransformer doesn't support out of vocabulary words,\n",
    "    hence we roll out our own.\n",
    "    All the parameters are gensim.models.Word2Vec's parameters.\n",
    "    https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None,\n",
    "                 sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5,\n",
    "                 ns_exponent=0.75, cbow_mean=1, hashfxn=hash, epochs=5, null_word=0,\n",
    "                 trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False,\n",
    "                 callbacks=(), max_final_vocab=None):\n",
    "        self.vector_size = vector_size\n",
    "        self.alpha = alpha\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.sample = sample\n",
    "        self.seed = seed\n",
    "        self.workers = workers\n",
    "        self.min_alpha = min_alpha\n",
    "        self.sg = sg\n",
    "        self.hs = hs\n",
    "        self.negative = negative\n",
    "        self.ns_exponent = ns_exponent\n",
    "        self.cbow_mean = cbow_mean\n",
    "        self.hashfxn = hashfxn\n",
    "        self.epochs = epochs\n",
    "        self.null_word = null_word\n",
    "        self.trim_rule = trim_rule\n",
    "        self.sorted_vocab = sorted_vocab\n",
    "        self.batch_words = batch_words\n",
    "        self.compute_loss = compute_loss\n",
    "        self.callbacks = callbacks\n",
    "        self.max_final_vocab = max_final_vocab\n",
    "    \n",
    "    def _get_embedding(self, words):\n",
    "        valid_words = [word for word in words if word in self.model_.wv.key_to_index]\n",
    "        if valid_words:\n",
    "            embedding = np.zeros((len(valid_words), self.vector_size), dtype=np.float32)\n",
    "            for idx, word in enumerate(valid_words):\n",
    "                embedding[idx] = self.model_.wv[word]\n",
    "\n",
    "            return np.mean(embedding, axis=0)\n",
    "        else:\n",
    "            return np.zeros(self.vector_size)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.model_ = Word2Vec(\n",
    "            sentences=X, corpus_file=None,\n",
    "            vector_size=self.vector_size, alpha=self.alpha, window=self.window, min_count=self.min_count,\n",
    "            max_vocab_size=self.max_vocab_size, sample=self.sample, seed=self.seed,\n",
    "            workers=self.workers, min_alpha=self.min_alpha, sg=self.sg, hs=self.hs,\n",
    "            negative=self.negative, ns_exponent=self.ns_exponent, cbow_mean=self.cbow_mean,\n",
    "            hashfxn=self.hashfxn, epochs=self.epochs, null_word=self.null_word,\n",
    "            trim_rule=self.trim_rule, sorted_vocab=self.sorted_vocab, batch_words=self.batch_words,\n",
    "            compute_loss=self.compute_loss, callbacks=self.callbacks,\n",
    "            max_final_vocab=self.max_final_vocab)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_embeddings = np.array([self._get_embedding(words) for words in X])\n",
    "        return X_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "557e02af-7cdd-4ec9-9a5e-af617c24966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_word2vec_tr = GensimWord2VecVectorizer(vector_size=100, min_count=1, sg=1, alpha=0.01, epochs=5)\n",
    "# xgb = XGBClassifier(learning_rate=0.01, n_estimators=100, n_jobs=-1)\n",
    "lr = LogisticRegression(max_iter=10_000)\n",
    "# rf = RandomForestClassifier(n_estimators=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3f235c4-848e-46d7-89d3-0c9d765e50b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 10:14:22: collecting all words and their counts\n",
      "INFO - 10:14:22: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 10:14:22: collected 26602 word types from a corpus of 497428 raw words and 10000 sentences\n",
      "INFO - 10:14:22: Creating a fresh vocabulary\n",
      "INFO - 10:14:22: Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 26602 unique words (100.00% of original 26602, drops 0)', 'datetime': '2022-08-31T10:14:22.208514', 'gensim': '4.2.0', 'python': '3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21) \\n[GCC 10.3.0]', 'platform': 'Linux-4.14.287-215.504.amzn2.x86_64-x86_64-with-glibc2.31', 'event': 'prepare_vocab'}\n",
      "INFO - 10:14:22: Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 497428 word corpus (100.00% of original 497428, drops 0)', 'datetime': '2022-08-31T10:14:22.209502', 'gensim': '4.2.0', 'python': '3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21) \\n[GCC 10.3.0]', 'platform': 'Linux-4.14.287-215.504.amzn2.x86_64-x86_64-with-glibc2.31', 'event': 'prepare_vocab'}\n",
      "INFO - 10:14:22: deleting the raw counts dictionary of 26602 items\n",
      "INFO - 10:14:22: sample=0.001 downsamples 51 most-common words\n",
      "INFO - 10:14:22: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 390021.2186910945 word corpus (78.4%% of prior 497428)', 'datetime': '2022-08-31T10:14:22.387173', 'gensim': '4.2.0', 'python': '3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21) \\n[GCC 10.3.0]', 'platform': 'Linux-4.14.287-215.504.amzn2.x86_64-x86_64-with-glibc2.31', 'event': 'prepare_vocab'}\n",
      "INFO - 10:14:22: estimated required memory for 26602 words and 100 dimensions: 34582600 bytes\n",
      "INFO - 10:14:22: resetting layer weights\n",
      "INFO - 10:14:22: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-08-31T10:14:22.647268', 'gensim': '4.2.0', 'python': '3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21) \\n[GCC 10.3.0]', 'platform': 'Linux-4.14.287-215.504.amzn2.x86_64-x86_64-with-glibc2.31', 'event': 'build_vocab'}\n",
      "INFO - 10:14:22: Word2Vec lifecycle event {'msg': 'training model with 3 workers on 26602 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-08-31T10:14:22.648448', 'gensim': '4.2.0', 'python': '3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21) \\n[GCC 10.3.0]', 'platform': 'Linux-4.14.287-215.504.amzn2.x86_64-x86_64-with-glibc2.31', 'event': 'train'}\n",
      "INFO - 10:14:23: EPOCH 0 - PROGRESS: at 56.38% examples, 213705 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 10:14:24: EPOCH 0: training on 497428 raw words (389798 effective words) took 1.6s, 239827 effective words/s\n",
      "INFO - 10:14:25: EPOCH 1 - PROGRESS: at 68.10% examples, 249917 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 10:14:25: EPOCH 1: training on 497428 raw words (389995 effective words) took 1.5s, 261670 effective words/s\n",
      "INFO - 10:14:26: EPOCH 2 - PROGRESS: at 64.22% examples, 249214 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 10:14:27: EPOCH 2: training on 497428 raw words (390077 effective words) took 1.5s, 260064 effective words/s\n",
      "INFO - 10:14:28: EPOCH 3 - PROGRESS: at 66.28% examples, 252278 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 10:14:28: EPOCH 3: training on 497428 raw words (389939 effective words) took 1.5s, 257056 effective words/s\n",
      "INFO - 10:14:29: EPOCH 4 - PROGRESS: at 68.32% examples, 251141 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 10:14:30: EPOCH 4: training on 497428 raw words (389696 effective words) took 1.5s, 260186 effective words/s\n",
      "INFO - 10:14:30: Word2Vec lifecycle event {'msg': 'training on 2487140 raw words (1949505 effective words) took 7.7s, 254831 effective words/s', 'datetime': '2022-08-31T10:14:30.299271', 'gensim': '4.2.0', 'python': '3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21) \\n[GCC 10.3.0]', 'platform': 'Linux-4.14.287-215.504.amzn2.x86_64-x86_64-with-glibc2.31', 'event': 'train'}\n",
      "INFO - 10:14:30: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=26602, vector_size=100, alpha=0.01>', 'datetime': '2022-08-31T10:14:30.300000', 'gensim': '4.2.0', 'python': '3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21) \\n[GCC 10.3.0]', 'platform': 'Linux-4.14.287-215.504.amzn2.x86_64-x86_64-with-glibc2.31', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.8 s, sys: 446 ms, total: 45.3 s\n",
      "Wall time: 30.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipeline1 = Pipeline([\n",
    "        ('w2v', gensim_word2vec_tr), \n",
    "        ('LogReg', lr)\n",
    "    ])\n",
    "pipeline1.fit(train_lists, train_label)\n",
    "oof_name = 'predicted_target'\n",
    "test_df[oof_name] = pipeline1.predict_proba(test_lists)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6388cfa-520f-4daa-92f5-33ecc68f16fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subgroup</th>\n",
       "      <th>subgroup_size</th>\n",
       "      <th>subgroup_auc</th>\n",
       "      <th>bpsn_auc</th>\n",
       "      <th>bnsp_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>homosexual_gay_or_lesbian</td>\n",
       "      <td>1065</td>\n",
       "      <td>0.574080</td>\n",
       "      <td>0.592646</td>\n",
       "      <td>0.610731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>muslim</td>\n",
       "      <td>2040</td>\n",
       "      <td>0.574092</td>\n",
       "      <td>0.644900</td>\n",
       "      <td>0.556778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>white</td>\n",
       "      <td>2452</td>\n",
       "      <td>0.602472</td>\n",
       "      <td>0.556341</td>\n",
       "      <td>0.663755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>christian</td>\n",
       "      <td>4226</td>\n",
       "      <td>0.610531</td>\n",
       "      <td>0.651460</td>\n",
       "      <td>0.581148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>4386</td>\n",
       "      <td>0.616764</td>\n",
       "      <td>0.580403</td>\n",
       "      <td>0.654694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>black</td>\n",
       "      <td>1519</td>\n",
       "      <td>0.623669</td>\n",
       "      <td>0.622352</td>\n",
       "      <td>0.621023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jewish</td>\n",
       "      <td>835</td>\n",
       "      <td>0.638669</td>\n",
       "      <td>0.635322</td>\n",
       "      <td>0.620466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>5155</td>\n",
       "      <td>0.643820</td>\n",
       "      <td>0.628806</td>\n",
       "      <td>0.630476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>psychiatric_or_mental_illness</td>\n",
       "      <td>511</td>\n",
       "      <td>0.665188</td>\n",
       "      <td>0.609278</td>\n",
       "      <td>0.671313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        subgroup  subgroup_size  subgroup_auc  bpsn_auc  \\\n",
       "2      homosexual_gay_or_lesbian           1065      0.574080  0.592646   \n",
       "5                         muslim           2040      0.574092  0.644900   \n",
       "7                          white           2452      0.602472  0.556341   \n",
       "3                      christian           4226      0.610531  0.651460   \n",
       "0                           male           4386      0.616764  0.580403   \n",
       "6                          black           1519      0.623669  0.622352   \n",
       "4                         jewish            835      0.638669  0.635322   \n",
       "1                         female           5155      0.643820  0.628806   \n",
       "8  psychiatric_or_mental_illness            511      0.665188  0.609278   \n",
       "\n",
       "   bnsp_auc  \n",
       "2  0.610731  \n",
       "5  0.556778  \n",
       "7  0.663755  \n",
       "3  0.581148  \n",
       "0  0.654694  \n",
       "6  0.621023  \n",
       "4  0.620466  \n",
       "1  0.630476  \n",
       "8  0.671313  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUBGROUP_AUC = 'subgroup_auc'\n",
    "BPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\n",
    "BNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n",
    "\n",
    "def compute_auc(y_true, y_pred):\n",
    "    try:\n",
    "        return roc_auc_score(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "def compute_subgroup_auc(df, subgroup, label, oof_name):\n",
    "    subgroup_examples = df[df[subgroup]]\n",
    "    return compute_auc(subgroup_examples[label], subgroup_examples[oof_name])\n",
    "\n",
    "def compute_bpsn_auc(df, subgroup, label, oof_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n",
    "    subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n",
    "    non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n",
    "    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
    "    return compute_auc(examples[label], examples[oof_name])\n",
    "\n",
    "def compute_bnsp_auc(df, subgroup, label, oof_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n",
    "    subgroup_positive_examples = df[df[subgroup] & df[label]]\n",
    "    non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n",
    "    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
    "    return compute_auc(examples[label], examples[oof_name])\n",
    "\n",
    "def compute_bias_metrics_for_model(dataset,\n",
    "                                   subgroups,\n",
    "                                   model,\n",
    "                                   label_col,\n",
    "                                   include_asegs=False):\n",
    "    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n",
    "    records = []\n",
    "    for subgroup in subgroups:\n",
    "        record = {\n",
    "            'subgroup': subgroup,\n",
    "            'subgroup_size': len(dataset[dataset[subgroup]])\n",
    "        }\n",
    "        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n",
    "        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n",
    "        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n",
    "        records.append(record)\n",
    "    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n",
    "oof_name = 'predicted_target'\n",
    "bias_metrics_df = compute_bias_metrics_for_model(test_df, identity_columns, oof_name, 'toxicity')\n",
    "bias_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b834c8d-2e72-4d34-87b6-dc131451c76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL SCORE IS 0.6148400353391639\n"
     ]
    }
   ],
   "source": [
    "def calculate_overall_auc(df, oof_name):\n",
    "    true_labels = df['toxicity']\n",
    "    predicted_labels = df[oof_name]\n",
    "    return roc_auc_score(true_labels, predicted_labels)\n",
    "\n",
    "def power_mean(series, p):\n",
    "    total = sum(np.power(series, p))\n",
    "    return np.power(total / len(series), 1 / p)\n",
    "\n",
    "def get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n",
    "    bias_score = np.average([\n",
    "        power_mean(bias_df[SUBGROUP_AUC], POWER),\n",
    "        power_mean(bias_df[BPSN_AUC], POWER),\n",
    "        power_mean(bias_df[BNSP_AUC], POWER)\n",
    "    ])\n",
    "    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n",
    "FINAL_SCORE = get_final_metric(bias_metrics_df, calculate_overall_auc(test_df, oof_name))\n",
    "print(f\"FINAL SCORE IS {FINAL_SCORE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cef07a0-aeb6-4dd7-b943-830713a47c1d",
   "metadata": {},
   "source": [
    "## Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2bb6352-451c-4790-8f7e-7e4d0582afc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 10:15:50: loading projection weights from /home/studio-lab-user/gensim-data/glove-wiki-gigaword-50/glove-wiki-gigaword-50.gz\n",
      "INFO - 10:16:04: KeyedVectors lifecycle event {'msg': 'loaded (400000, 50) matrix of type float32 from /home/studio-lab-user/gensim-data/glove-wiki-gigaword-50/glove-wiki-gigaword-50.gz', 'binary': False, 'encoding': 'utf8', 'datetime': '2022-08-31T10:16:04.842871', 'gensim': '4.2.0', 'python': '3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21) \\n[GCC 10.3.0]', 'platform': 'Linux-4.14.287-215.504.amzn2.x86_64-x86_64-with-glibc2.31', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "w2v_model = api.load(\"glove-wiki-gigaword-50\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42cd8586-c127-4f41-8376-ec41a8e95ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlovoWord2VecVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Word vectors are averaged across to create the document-level vectors/features.\n",
    "    gensim's own gensim.sklearn_api.W2VTransformer doesn't support out of vocabulary words,\n",
    "    hence we roll out our own.\n",
    "    All the parameters are gensim.models.Word2Vec's parameters.\n",
    "    https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.vector_size = model.vector_size\n",
    "\n",
    "    def fit(self, X, y): # what are X and y?\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_embeddings = np.array([self._get_embedding(words) for words in tqdm(X)])\n",
    "        return X_embeddings\n",
    "\n",
    "    def _get_embedding(self, words):\n",
    "        valid_words = [word for word in words if word in self.model.key_to_index]\n",
    "        if valid_words:\n",
    "            embedding = np.zeros((len(valid_words), self.vector_size), dtype=np.float32)\n",
    "            for idx, word in enumerate(valid_words):\n",
    "                embedding[idx] = self.model[word]\n",
    "\n",
    "            return np.mean(embedding, axis=0)\n",
    "        else:\n",
    "            return np.zeros(self.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ceb02e3-262c-4aff-acc5-7bceca7718c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "glovo_word2vec_tr = GlovoWord2VecVectorizer(w2v_model)\n",
    "# xgb = XGBClassifier(learning_rate=0.01, n_estimators=100, n_jobs=-1)\n",
    "lg = LogisticRegression(max_iter=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6333006f-1580-4c17-ab6d-e0b0b42c4a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 6147.18it/s]\n",
      "100%|██████████| 194641/194641 [00:22<00:00, 8817.64it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.4 s, sys: 700 ms, total: 25.1 s\n",
      "Wall time: 25.7 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subgroup</th>\n",
       "      <th>subgroup_size</th>\n",
       "      <th>subgroup_auc</th>\n",
       "      <th>bpsn_auc</th>\n",
       "      <th>bnsp_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>homosexual_gay_or_lesbian</td>\n",
       "      <td>1065</td>\n",
       "      <td>0.668843</td>\n",
       "      <td>0.655776</td>\n",
       "      <td>0.786936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>black</td>\n",
       "      <td>1519</td>\n",
       "      <td>0.711937</td>\n",
       "      <td>0.627373</td>\n",
       "      <td>0.830535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>white</td>\n",
       "      <td>2452</td>\n",
       "      <td>0.714141</td>\n",
       "      <td>0.618394</td>\n",
       "      <td>0.838110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>muslim</td>\n",
       "      <td>2040</td>\n",
       "      <td>0.722663</td>\n",
       "      <td>0.658737</td>\n",
       "      <td>0.817044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>4386</td>\n",
       "      <td>0.725412</td>\n",
       "      <td>0.607731</td>\n",
       "      <td>0.855115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>psychiatric_or_mental_illness</td>\n",
       "      <td>511</td>\n",
       "      <td>0.726380</td>\n",
       "      <td>0.746955</td>\n",
       "      <td>0.751674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>5155</td>\n",
       "      <td>0.735267</td>\n",
       "      <td>0.672384</td>\n",
       "      <td>0.818360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jewish</td>\n",
       "      <td>835</td>\n",
       "      <td>0.742281</td>\n",
       "      <td>0.663796</td>\n",
       "      <td>0.824916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>christian</td>\n",
       "      <td>4226</td>\n",
       "      <td>0.781263</td>\n",
       "      <td>0.749477</td>\n",
       "      <td>0.792213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        subgroup  subgroup_size  subgroup_auc  bpsn_auc  \\\n",
       "2      homosexual_gay_or_lesbian           1065      0.668843  0.655776   \n",
       "6                          black           1519      0.711937  0.627373   \n",
       "7                          white           2452      0.714141  0.618394   \n",
       "5                         muslim           2040      0.722663  0.658737   \n",
       "0                           male           4386      0.725412  0.607731   \n",
       "8  psychiatric_or_mental_illness            511      0.726380  0.746955   \n",
       "1                         female           5155      0.735267  0.672384   \n",
       "4                         jewish            835      0.742281  0.663796   \n",
       "3                      christian           4226      0.781263  0.749477   \n",
       "\n",
       "   bnsp_auc  \n",
       "2  0.786936  \n",
       "6  0.830535  \n",
       "7  0.838110  \n",
       "5  0.817044  \n",
       "0  0.855115  \n",
       "8  0.751674  \n",
       "1  0.818360  \n",
       "4  0.824916  \n",
       "3  0.792213  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#1_000_000\n",
    "\n",
    "pipeline2 = Pipeline([\n",
    "    ('glovo', glovo_word2vec_tr), \n",
    "    ('LogReg', lg)\n",
    "])\n",
    "pipeline2.fit(train_lists, train_label)\n",
    "oof_name = 'predicted_target_glovo'\n",
    "test_df[oof_name] = pipeline2.predict_proba(test_lists)[:, 1]\n",
    "bias_metrics_df = compute_bias_metrics_for_model(test_df, identity_columns, oof_name, 'toxicity')\n",
    "bias_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29a0b46a-64f9-42ea-8537-a5912d4846f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL SCORE IS 0.739308681899916\n"
     ]
    }
   ],
   "source": [
    "FINAL_SCORE = get_final_metric(bias_metrics_df, calculate_overall_auc(test_df, oof_name))\n",
    "print(f\"FINAL SCORE IS {FINAL_SCORE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5eac16-0198-4667-9f45-b17b3419343e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "godel:Python",
   "language": "python",
   "name": "conda-env-godel-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
